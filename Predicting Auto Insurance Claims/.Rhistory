test$predict_prob = test_pred_probs
#Took most of this next line from: https://www.r-bloggers.com/2020/05/binary-logistic-regression-with-r/
test$predicted =  as.factor(ifelse(test_pred_probs >= threshold, 1, 0))
return(test[,c("predict_prob","predicted")])
}
predictions <- make.predictions(model, validation, threshold = 0.50)
library(pROC)
confusionMatrix(predictions$predicted, as.factor(validation$TARGET_FLAG), positive = '1')
proc = roc(as.factor(validation$TARGET_FLAG), predictions$predict_prob)
plot(proc)
print(proc$auc)
#I'm copying alot of this from the last assignment
library(glmnet)  #Was a helpful guide: https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html
#There are random elements to this, but I'm not sure where (relaxed fit?)
set.seed(1234567890)
#Data prep.  Needs to be in matrix format
#Took code from here: https://stackoverflow.com/questions/35437411/error-in-predict-glmnet-function-not-yet-implemented-method
train_X <- model.matrix(~.-TARGET_FLAG -TARGET_AMT,data=train)
train_Y <- train$TARGET_FLAG
val_X = model.matrix(~.-TARGET_FLAG -TARGET_AMT,data=validation)
#Makes a series of crossvalidated glmnet models for 100 lambda values (default)
#lamba values are constants that define coefficient shrinkage.
LASSO_crash_model <- cv.glmnet(x = train_X,   #Predictor variables
y = train_Y,
family = "binomial", #Has it do logistic regression
nfolds = 20, #k fold cv
type.measure = "class",  #uses missclassification error as loss
alpha = 1) #Alpha = 1 is lasso.
#Predicts the probability that the target variable is 1
#setting lambda.min uses the lambda value with the minimum mean cv error (picks the best model)
predictions <- predict(LASSO_crash_model,
newx = val_X,
type = "response",
s=LASSO_crash_model$lambda.min)
#Print's the coefficients the model uses
print(coef.glmnet(LASSO_crash_model, s = LASSO_crash_model$lambda.min))
confusionMatrix(as.factor(ifelse(predictions >= 0.5, 1, 0)), as.factor(validation$TARGET_FLAG), positive = '1')
proc = roc(validation$TARGET_FLAG, predictions)
plot(proc)
print(proc$auc)
#Select only instances where a crash occured
all_crash <- subset(df, TARGET_FLAG == 1)
all_crash$TARGET_FLAG <- NULL
set.seed(0987654321)
#Train test split
splitdex <- createDataPartition(all_crash$TARGET_AMT, p = 0.8, list = FALSE)
crash_train <- all_crash[splitdex,]
crash_validation <- all_crash[-splitdex,]
base_model_payout <- lm(TARGET_AMT~., crash_train)
summary(base_model_payout)
plot(base_model_payout)
#Transforms the response variable first
bluebook_model_payout <- lm(log(crash_train$TARGET_AMT)~BLUEBOOK, crash_train)
summary(bluebook_model_payout)
plot(bluebook_model_payout)
#Finds how well the predictions match up to the actual validation data
#Also reverses the log transformation after predictions are made
fit <- lm(exp(predict(bluebook_model_payout, crash_validation[,2:24]))~crash_validation$TARGET_AMT)
plot(exp(predict(bluebook_model_payout, crash_validation[,2:24]))~crash_validation$TARGET_AMT)
abline(0,1, col = "red")
summary(fit)
set.seed(0987654321)
#Data prep.  Needs to be in matrix format
#Took code from here: https://stackoverflow.com/questions/35437411/error-in-predict-glmnet-function-not-yet-implemented-method
crash_train_X <- model.matrix(~. -TARGET_AMT,data=crash_train)
#Needs a log transformation
crash_train_Y <- log(crash_train$TARGET_AMT)
crash_val_X = model.matrix(~. -TARGET_AMT,data=crash_validation)
#Makes a series of crossvalidated glmnet models for 100 lambda values (default)
#lamba values are constants that define coefficient shrinkage.
LASSO_payout_model <- cv.glmnet(x = crash_train_X,   #Predictor variables
y = crash_train_Y,
nfolds = 10, #k fold cv
type.measure = "mse",  #uses mean squared error as loss
alpha = 1) #Alpha = 1 is lasso.
#setting lambda.min uses the lambda value with the minimum mean cv error (picks the best model).  also corrects for the log transformation
predictions <- exp(predict(LASSO_payout_model, newx = crash_val_X, s=LASSO_payout_model$lambda.min))
#Print's the coefficients the model uses
print(coef.glmnet(LASSO_payout_model, s = LASSO_payout_model$lambda.min))
plot(predictions~crash_validation$TARGET_AMT)
abline(0,1, col = "red")
fit <- lm(predictions~crash_validation$TARGET_AMT)
summary(fit)
url <- "https://raw.githubusercontent.com/davidblumenstiel/Portfolio_2022/main/Predicting%20Auto%20Insurance%20Claims/data/insurance-evaluation-data.csv"
test<- fetch_and_prep(url)
#Not sure why, but this needs slighly different prep than the validation set
test_X <- model.matrix(~.,data=test[,3:25])
#Predicts probability of crash
predictions <- data.frame(predict(LASSO_crash_model, newx = test_X, type = "response", s=LASSO_crash_model$lambda.min))
colnames(predictions) <- "crash_probability"
#If probability of a crash is >50% then it lists as crash (1), otherwise no crash (0)
predictions <- predictions %>%
mutate(class = as.factor(round(crash_probability)))
#If there's a crash, then it assigns a predicted cost for the payout, otherwise it sets it to 0
#There are probably better ways to do this than overwriting
predictions["cost"] <- exp(predict(bluebook_model_payout, test))
predictions$cost[predictions$class == 0] <- 0
#Save predictions locally
write.csv(predictions, file = "predictions.csv")
View(raw)
View(raw)
View(raw)
head(raw)
corrplot(cor(df[,c(2,4,6,7,9,14,16,17,20,23,24)], method = "pearson"), method = "square")
View(train_X)
#I'm copying alot of this from a previous work of mine
library(glmnet)  #Was a helpful guide: https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html
#There are random elements to this, but I'm not sure where (relaxed fit?)
set.seed(1234567890)
#Data prep.  Needs to be in matrix format
#Took code from here: https://stackoverflow.com/questions/35437411/error-in-predict-glmnet-function-not-yet-implemented-method
train_X <- model.matrix(~.-TARGET_FLAG -TARGET_AMT,data=train)
train_Y <- train$TARGET_FLAG
val_X = model.matrix(~.-TARGET_FLAG -TARGET_AMT,data=validation)
#Makes a series of crossvalidated glmnet models for 100 lambda values (default)
#lamba values are constants that define coefficient shrinkage.
LASSO_crash_model <- cv.glmnet(x = train_X,   #Predictor variables
y = train_Y,
family = "binomial", #Has it use logit
nfolds = 20, #k fold cv
intercept = TRUE,
standardize = TRUE,
type.measure = "class",  #uses missclassification error as loss
alpha = 1) #Alpha = 1 is lasso.
#Predicts the probability that the target variable is 1
#setting lambda.min uses the lambda value with the minimum mean cv error (picks the best model)
predictions <- predict(LASSO_crash_model,
newx = val_X,
type = "response",
s=LASSO_crash_model$lambda.min)
#Print's the coefficients the model uses
print(coef.glmnet(LASSO_crash_model, s = LASSO_crash_model$lambda.min))
# Set so that long lines will be wrapped for pdfs:
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), tidy=TRUE)
library(tidyr)
library(dplyr)
library(caret)
library(corrplot)
library(pROC)
#Data import
#Some of the missing data is written as blank instead of NA.  na.strings takes care of that
raw <- read.csv("https://raw.githubusercontent.com/davidblumenstiel/Portfolio_2022/main/Predicting%20Auto%20Insurance%20Claims/data/insurance_training_data.csv", na.strings=c(""," "))
summary(raw)
head(raw)
fetch_and_prep <- function(url) { #Will take a url and return the prepaired dataset
#Some of the missing data is written as blank instead of NA.  na.strings takes care of that
df = read.csv(url, na.strings=c(""," "))
#Scrap the index variable
df$INDEX <- NULL
#Change to factor where appropriate
df[c("KIDSDRIV","HOMEKIDS","CLM_FREQ")] = lapply(df[c("KIDSDRIV","HOMEKIDS","CLM_FREQ")],factor)
#Change to numeric where appropriate by first converting to characters, then removing '$' and ',', and then converting to numeric
df[c("INCOME","HOME_VAL","BLUEBOOK","OLDCLAIM")] = lapply(df[c("INCOME","HOME_VAL","BLUEBOOK","OLDCLAIM")], function(x) as.numeric(gsub('[,]','',gsub('[$]','',as.character(x)))))
#Adds some more levels to factors so train and test sets have the same categorical variables
if( "4" %in% levels(df$KIDSDRIV) == FALSE) {  #from: https://stackoverflow.com/questions/40034750/how-to-check-if-a-factor-variable-has-only-the-levels-i-want
levels(df$KIDSDRIV) = c(levels(df$KIDSDRIV),"4")
}
#############
#NA Imputation
#Definitely up for debate as to how to handle missing data here.  Here's one take:
#Could also definitely use regression to impute alot of this (would probably be the better option), but this is less complex
#Income: will set to median of job type.  If job is also NA, it assumes no job and income is 0
levels(df$JOB) = c(levels(df$JOB),"Unemployed","Unlisted") #adds some more job options
incomes = aggregate(INCOME~JOB, df, median)
i = 0
for(val in df$INCOME){
i = i+1
if(is.na(val)) {
if(is.na(df[i,"JOB"])) {
df[i,"INCOME"] = 0
#Will also change job type to unemployed if no income or job listed
df[i,"JOB"] = 'Unemployed'
}
else{
df[i,"INCOME"] = incomes$INCOME[incomes$JOB == df[i,"JOB"]]
}
}
}
#Job type: if job is NA but income is 0<, then it's likely they are employed; set job to 'unlisted'
df$JOB[is.na(df$JOB)] = "Unlisted"
#Age: Set's it to median.  Not many NA's here
df$AGE[is.na(df$AGE)] = median(df$AGE, na.rm = TRUE)
#Years on job: Set to median of that type of job
yearsonjob = aggregate(YOJ~JOB, df, median)
i = 0
for(val in df$YOJ){
i = i+1
if(is.na(val)) {
df[i,"YOJ"] = yearsonjob$YOJ[yearsonjob$JOB == df[i,"JOB"]]
}
}
#Home value: Will assume NA means 0 home value (does not own home).  This one is up for debate
df$HOME_VAL[is.na(df$HOME_VAL)] = 0
#Car Age.  Will set it to the median age of that type of car.  Linear regression using bluebook and cartype would be better
carages = aggregate(CAR_AGE~CAR_TYPE, df, median)
i = 0
for(val in df$CAR_AGE){
i = i+1
if(is.na(val)) {
df[i,"CAR_AGE"] = carages$CAR_AGE[carages$CAR_TYPE == df[i,"CAR_TYPE"]]
}
if(df[i,"CAR_AGE"] < 0) { #Someone set their car age to -3 in the training set
df[i,"CAR_AGE"] = 0
}
}
return(df)
}
url <- "https://raw.githubusercontent.com/davidblumenstiel/Portfolio_2022/main/Predicting%20Auto%20Insurance%20Claims/data/insurance_training_data.csv"
df <- fetch_and_prep(url)
summary(df)
corrplot(cor(df[,c(2,4,6,7,9,14,16,17,20,23,24)], method = "pearson"), method = "square")
#Train test split
set.seed(1234567890)
splitdex<- createDataPartition(df$TARGET_FLAG, p = 0.8, list = FALSE)
train <- df[splitdex,]
validation <- df[-splitdex,]
#Make the model
model <- glm(TARGET_FLAG~.-TARGET_AMT, data = train, family = "binomial")
summary(model)
make.predictions <- function(model, test, threshold = 0.5) {
test_pred_probs = predict(model, test, type = "response")
test$predict_prob = test_pred_probs
#Took most of this next line from: https://www.r-bloggers.com/2020/05/binary-logistic-regression-with-r/
test$predicted =  as.factor(ifelse(test_pred_probs >= threshold, 1, 0))
return(test[,c("predict_prob","predicted")])
}
predictions <- make.predictions(model, validation, threshold = 0.50)
confusionMatrix(predictions$predicted, as.factor(validation$TARGET_FLAG), positive = '1')
proc = roc(as.factor(validation$TARGET_FLAG), predictions$predict_prob)
plot(proc)
print(proc$auc)
#I'm copying alot of this from a previous work of mine
library(glmnet)  #Was a helpful guide: https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html
#There are random elements to this, but I'm not sure where (relaxed fit?)
set.seed(1234567890)
#Data prep.  Needs to be in matrix format
#Took code from here: https://stackoverflow.com/questions/35437411/error-in-predict-glmnet-function-not-yet-implemented-method
train_X <- model.matrix(~.-TARGET_FLAG -TARGET_AMT,data=train)
train_Y <- train$TARGET_FLAG
val_X = model.matrix(~.-TARGET_FLAG -TARGET_AMT,data=validation)
#Makes a series of crossvalidated glmnet models for 100 lambda values (default)
#lamba values are constants that define coefficient shrinkage.
LASSO_crash_model <- cv.glmnet(x = train_X,   #Predictor variables
y = train_Y,
family = "binomial", #Has it use logit
nfolds = 20, #k fold cv
intercept = TRUE,
standardize = TRUE,
type.measure = "class",  #uses missclassification error as loss
alpha = 1) #Alpha = 1 is lasso.
#Predicts the probability that the target variable is 1
#setting lambda.min uses the lambda value with the minimum mean cv error (picks the best model)
predictions <- predict(LASSO_crash_model,
newx = val_X,
type = "response",
s=LASSO_crash_model$lambda.min)
#Print's the coefficients the model uses
print(coef.glmnet(LASSO_crash_model, s = LASSO_crash_model$lambda.min))
confusionMatrix(as.factor(ifelse(predictions >= 0.5, 1, 0)), as.factor(validation$TARGET_FLAG), positive = '1')
proc = roc(validation$TARGET_FLAG, predictions)
plot(proc)
print(proc$auc)
#Select only instances where a crash occured
all_crash <- subset(df, TARGET_FLAG == 1)
all_crash$TARGET_FLAG <- NULL
set.seed(0987654321)
#Train test split
splitdex <- createDataPartition(all_crash$TARGET_AMT, p = 0.8, list = FALSE)
crash_train <- all_crash[splitdex,]
crash_validation <- all_crash[-splitdex,]
base_model_payout <- lm(TARGET_AMT~., crash_train)
summary(base_model_payout)
plot(base_model_payout)
#Transforms the response variable first
bluebook_model_payout <- lm(log(crash_train$TARGET_AMT)~BLUEBOOK, crash_train)
summary(bluebook_model_payout)
plot(bluebook_model_payout)
#Finds how well the predictions match up to the actual validation data
#Also reverses the log transformation after predictions are made
fit <- lm(exp(predict(bluebook_model_payout, crash_validation[,2:24]))~crash_validation$TARGET_AMT)
plot(exp(predict(bluebook_model_payout, crash_validation[,2:24]))~crash_validation$TARGET_AMT)
abline(0,1, col = "red")
summary(fit)
set.seed(0987654321)
#Data prep.  Needs to be in matrix format
#Took code from here: https://stackoverflow.com/questions/35437411/error-in-predict-glmnet-function-not-yet-implemented-method
crash_train_X <- model.matrix(~. -TARGET_AMT,data=crash_train)
#Needs a log transformation
crash_train_Y <- log(crash_train$TARGET_AMT)
crash_val_X = model.matrix(~. -TARGET_AMT,data=crash_validation)
#Makes a series of crossvalidated glmnet models for 100 lambda values (default)
#lamba values are constants that define coefficient shrinkage.
LASSO_payout_model <- cv.glmnet(x = crash_train_X,   #Predictor variables
y = crash_train_Y,
nfolds = 10, #k fold cv
type.measure = "mse",  #uses mean squared error as loss
alpha = 1) #Alpha = 1 is lasso.
#setting lambda.min uses the lambda value with the minimum mean cv error (picks the best model).  also corrects for the log transformation
predictions <- exp(predict(LASSO_payout_model, newx = crash_val_X, s=LASSO_payout_model$lambda.min))
#Print's the coefficients the model uses
print(coef.glmnet(LASSO_payout_model, s = LASSO_payout_model$lambda.min))
plot(predictions~crash_validation$TARGET_AMT)
abline(0,1, col = "red")
fit <- lm(predictions~crash_validation$TARGET_AMT)
summary(fit)
url <- "https://raw.githubusercontent.com/davidblumenstiel/Portfolio_2022/main/Predicting%20Auto%20Insurance%20Claims/data/insurance-evaluation-data.csv"
test<- fetch_and_prep(url)
#Not sure why, but this needs slighly different prep than the validation set
test_X <- model.matrix(~.,data=test[,3:25])
#Predicts probability of crash
predictions <- data.frame(predict(LASSO_crash_model, newx = test_X, type = "response", s=LASSO_crash_model$lambda.min))
colnames(predictions) <- "crash_probability"
#If probability of a crash is >50% then it lists as crash (1), otherwise no crash (0)
predictions <- predictions %>%
mutate(class = as.factor(round(crash_probability)))
#If there's a crash, then it assigns a predicted cost for the payout, otherwise it sets it to 0
#There are probably better ways to do this than overwriting
predictions["cost"] <- exp(predict(bluebook_model_payout, test))
predictions$cost[predictions$class == 0] <- 0
#Save predictions locally
write.csv(predictions, file = "predictions.csv")
# Set so that long lines will be wrapped for pdfs:
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), tidy=TRUE)
library(tidyr)
library(dplyr)
library(caret)
library(corrplot)
library(pROC)
#Data import
#Some of the missing data is written as blank instead of NA.  na.strings takes care of that
raw <- read.csv("https://raw.githubusercontent.com/davidblumenstiel/Portfolio_2022/main/Predicting%20Auto%20Insurance%20Claims/data/insurance_training_data.csv", na.strings=c(""," "))
summary(raw)
head(raw)
fetch_and_prep <- function(url) { #Will take a url and return the prepaired dataset
#Some of the missing data is written as blank instead of NA.  na.strings takes care of that
df = read.csv(url, na.strings=c(""," "))
#Scrap the index variable
df$INDEX <- NULL
#Change to factor where appropriate
df[c("KIDSDRIV","HOMEKIDS","CLM_FREQ")] = lapply(df[c("KIDSDRIV","HOMEKIDS","CLM_FREQ")],factor)
#Change to numeric where appropriate by first converting to characters, then removing '$' and ',', and then converting to numeric
df[c("INCOME","HOME_VAL","BLUEBOOK","OLDCLAIM")] = lapply(df[c("INCOME","HOME_VAL","BLUEBOOK","OLDCLAIM")], function(x) as.numeric(gsub('[,]','',gsub('[$]','',as.character(x)))))
#Adds some more levels to factors so train and test sets have the same categorical variables
if( "4" %in% levels(df$KIDSDRIV) == FALSE) {  #from: https://stackoverflow.com/questions/40034750/how-to-check-if-a-factor-variable-has-only-the-levels-i-want
levels(df$KIDSDRIV) = c(levels(df$KIDSDRIV),"4")
}
#############
#NA Imputation
#Definitely up for debate as to how to handle missing data here.  Here's one take:
#Could also definitely use regression to impute alot of this (would probably be the better option), but this is less complex
#Income: will set to median of job type.  If job is also NA, it assumes no job and income is 0
levels(df$JOB) = c(levels(df$JOB),"Unemployed","Unlisted") #adds some more job options
incomes = aggregate(INCOME~JOB, df, median)
i = 0
for(val in df$INCOME){
i = i+1
if(is.na(val)) {
if(is.na(df[i,"JOB"])) {
df[i,"INCOME"] = 0
#Will also change job type to unemployed if no income or job listed
df[i,"JOB"] = 'Unemployed'
}
else{
df[i,"INCOME"] = incomes$INCOME[incomes$JOB == df[i,"JOB"]]
}
}
}
#Job type: if job is NA but income is 0<, then it's likely they are employed; set job to 'unlisted'
df$JOB[is.na(df$JOB)] = "Unlisted"
#Age: Set's it to median.  Not many NA's here
df$AGE[is.na(df$AGE)] = median(df$AGE, na.rm = TRUE)
#Years on job: Set to median of that type of job
yearsonjob = aggregate(YOJ~JOB, df, median)
i = 0
for(val in df$YOJ){
i = i+1
if(is.na(val)) {
df[i,"YOJ"] = yearsonjob$YOJ[yearsonjob$JOB == df[i,"JOB"]]
}
}
#Home value: Will assume NA means 0 home value (does not own home).  This one is up for debate
df$HOME_VAL[is.na(df$HOME_VAL)] = 0
#Car Age.  Will set it to the median age of that type of car.  Linear regression using bluebook and cartype would be better
carages = aggregate(CAR_AGE~CAR_TYPE, df, median)
i = 0
for(val in df$CAR_AGE){
i = i+1
if(is.na(val)) {
df[i,"CAR_AGE"] = carages$CAR_AGE[carages$CAR_TYPE == df[i,"CAR_TYPE"]]
}
if(df[i,"CAR_AGE"] < 0) { #Someone set their car age to -3 in the training set
df[i,"CAR_AGE"] = 0
}
}
return(df)
}
url <- "https://raw.githubusercontent.com/davidblumenstiel/Portfolio_2022/main/Predicting%20Auto%20Insurance%20Claims/data/insurance_training_data.csv"
df <- fetch_and_prep(url)
summary(df)
corrplot(cor(df[,c(2,4,6,7,9,14,16,17,20,23,24)], method = "pearson"), method = "square")
#Train test split
set.seed(1234567890)
splitdex<- createDataPartition(df$TARGET_FLAG, p = 0.8, list = FALSE)
train <- df[splitdex,]
validation <- df[-splitdex,]
#Make the model
model <- glm(TARGET_FLAG~.-TARGET_AMT, data = train, family = "binomial")
summary(model)
make.predictions <- function(model, test, threshold = 0.5) {
test_pred_probs = predict(model, test, type = "response")
test$predict_prob = test_pred_probs
#Took most of this next line from: https://www.r-bloggers.com/2020/05/binary-logistic-regression-with-r/
test$predicted =  as.factor(ifelse(test_pred_probs >= threshold, 1, 0))
return(test[,c("predict_prob","predicted")])
}
predictions <- make.predictions(model, validation, threshold = 0.50)
confusionMatrix(predictions$predicted, as.factor(validation$TARGET_FLAG), positive = '1')
proc = roc(as.factor(validation$TARGET_FLAG), predictions$predict_prob)
plot(proc)
print(proc$auc)
#I'm copying alot of this from a previous work of mine
library(glmnet)  #Was a helpful guide: https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html
#There are random elements to this, but I'm not sure where (relaxed fit?)
set.seed(1234567890)
#Data prep.  Needs to be in matrix format
#Took code from here: https://stackoverflow.com/questions/35437411/error-in-predict-glmnet-function-not-yet-implemented-method
train_X <- model.matrix(~.-TARGET_FLAG -TARGET_AMT,data=train)
train_Y <- train$TARGET_FLAG
val_X = model.matrix(~.-TARGET_FLAG -TARGET_AMT,data=validation)
#Makes a series of crossvalidated glmnet models for 100 lambda values (default)
#lamba values are constants that define coefficient shrinkage.
LASSO_crash_model <- cv.glmnet(x = train_X,   #Predictor variables
y = train_Y,
family = "binomial", #Has it use logit
nfolds = 20, #k fold cv
intercept = TRUE,
standardize = TRUE,
type.measure = "class",  #uses missclassification error as loss
alpha = 1) #Alpha = 1 is lasso.
#Predicts the probability that the target variable is 1
#setting lambda.min uses the lambda value with the minimum mean cv error (picks the best model)
predictions <- predict(LASSO_crash_model,
newx = val_X,
type = "response",
s=LASSO_crash_model$lambda.min)
#Print's the coefficients the model uses
print(coef.glmnet(LASSO_crash_model, s = LASSO_crash_model$lambda.min))
confusionMatrix(as.factor(ifelse(predictions >= 0.5, 1, 0)), as.factor(validation$TARGET_FLAG), positive = '1')
proc = roc(validation$TARGET_FLAG, predictions)
plot(proc)
print(proc$auc)
#Select only instances where a crash occured
all_crash <- subset(df, TARGET_FLAG == 1)
all_crash$TARGET_FLAG <- NULL
set.seed(0987654321)
#Train test split
splitdex <- createDataPartition(all_crash$TARGET_AMT, p = 0.8, list = FALSE)
crash_train <- all_crash[splitdex,]
crash_validation <- all_crash[-splitdex,]
base_model_payout <- lm(TARGET_AMT~., crash_train)
summary(base_model_payout)
plot(base_model_payout)
#Transforms the response variable first
bluebook_model_payout <- lm(log(crash_train$TARGET_AMT)~BLUEBOOK, crash_train)
summary(bluebook_model_payout)
plot(bluebook_model_payout)
#Finds how well the predictions match up to the actual validation data
#Also reverses the log transformation after predictions are made
fit <- lm(exp(predict(bluebook_model_payout, crash_validation[,2:24]))~crash_validation$TARGET_AMT)
plot(exp(predict(bluebook_model_payout, crash_validation[,2:24]))~crash_validation$TARGET_AMT)
abline(0,1, col = "red")
summary(fit)
set.seed(0987654321)
#Data prep.  Needs to be in matrix format
#Took code from here: https://stackoverflow.com/questions/35437411/error-in-predict-glmnet-function-not-yet-implemented-method
crash_train_X <- model.matrix(~. -TARGET_AMT,data=crash_train)
#Needs a log transformation
crash_train_Y <- log(crash_train$TARGET_AMT)
crash_val_X = model.matrix(~. -TARGET_AMT,data=crash_validation)
#Makes a series of crossvalidated glmnet models for 100 lambda values (default)
#lamba values are constants that define coefficient shrinkage.
LASSO_payout_model <- cv.glmnet(x = crash_train_X,   #Predictor variables
y = crash_train_Y,
intercept = TRUE,
standardize = TRUE,
nfolds = 10, #k fold cv
type.measure = "mse",  #uses mean squared error as loss
alpha = 1) #Alpha = 1 is lasso.
#setting lambda.min uses the lambda value with the minimum mean cv error (picks the best model).  also corrects for the log transformation
predictions <- exp(predict(LASSO_payout_model, newx = crash_val_X, s=LASSO_payout_model$lambda.min))
#Print's the coefficients the model uses
print(coef.glmnet(LASSO_payout_model, s = LASSO_payout_model$lambda.min))
plot(predictions~crash_validation$TARGET_AMT)
abline(0,1, col = "red")
fit <- lm(predictions~crash_validation$TARGET_AMT)
summary(fit)
url <- "https://raw.githubusercontent.com/davidblumenstiel/Portfolio_2022/main/Predicting%20Auto%20Insurance%20Claims/data/insurance-evaluation-data.csv"
test<- fetch_and_prep(url)
#Not sure why, but this needs slighly different prep than the validation set
test_X <- model.matrix(~.,data=test[,3:25])
#Predicts probability of crash
predictions <- data.frame(predict(LASSO_crash_model, newx = test_X, type = "response", s=LASSO_crash_model$lambda.min))
colnames(predictions) <- "crash_probability"
#If probability of a crash is >50% then it lists as crash (1), otherwise no crash (0)
predictions <- predictions %>%
mutate(class = as.factor(round(crash_probability)))
#If there's a crash, then it assigns a predicted cost for the payout, otherwise it sets it to 0
#There are probably better ways to do this than overwriting
predictions["cost"] <- exp(predict(bluebook_model_payout, test))
predictions$cost[predictions$class == 0] <- 0
#Save predictions locally
write.csv(predictions, file = "predictions.csv")
