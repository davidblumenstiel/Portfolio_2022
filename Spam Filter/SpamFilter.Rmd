---
title: 'Spam Filter'
author: "David Blumenstiel"
date: "4/20/2020"
output:
  html_document:
    df_print: paged
  'output: pdf_document': default
  pdf_document: default
---

# No one likes spam email.  Thus, we have spam filters built in to our email services that detect and send spam into a deep dark folder where no one can hear it scream.  Let's make a spam detector.

### Let's start by loading the libraries we'll use.

```{r setup, message=FALSE} 

library("readtext")
library("tm")
library("dplyr")
library("ggplot2")
library("gridExtra")
library("caTools")
library("randomForest")
library("caret")
library("readr")
library("SnowballC")


```

### With that out of the way, the first task will be to load sets of spam and ham (non-spam) messages into a dataframe.  The data was sourced from:  https://spamassassin.apache.org/old/publiccorpus

### We will begin with two seperate folders containing spam and ham, and combine them together into a dataframe.  One column will have text, with each message as it's own observation, and a second column with spam/ham labeling.  The data was precompiled using the function below, and uploaded to GitHub.


```{r, message=FALSE, warning=FALSE}
#The function used to compile the data
#This won't actually run by default: the dataframe this creates is instead pre-loaded from github 
compile_spamham <- function(spamdir, hamdir) {
  
  #Initializes an empty dataframe
  spamhamdocs <- data.frame(body = character(), is_spam = logical(), stringsAsFactors = FALSE)
  
  #Returns a list of file names for all of the files in the folder where we keep the spam emails
  filenames <- list.files(spamdir)
  
  #Populates the dataframe with the spam messages, and labels them spam
  i = 0
  while (i < length(filenames)) {
    i = i+1
    spamhamdocs[nrow(spamhamdocs) + 1,1] <- suppressWarnings(readtext(paste0(spamdir,"\\",filenames[i])))[2]
    spamhamdocs[nrow(spamhamdocs),2] <- TRUE
  }
  
  #Populates the dataframe with the ham messages, and labels them as not spam (same as above)
  filenames <- list.files(hamdir)
  i = 0
  while (i < length(filenames)) {
    i = i+1
    spamhamdocs[nrow(spamhamdocs) + 1,1] <- suppressWarnings(readtext(paste0(hamdir,"\\",filenames[i])))[2]
    spamhamdocs[nrow(spamhamdocs),2] <- FALSE
    
  }
  
  #writes the compiled spam/ham data to a csv
  write.csv(spamhamdocs,
            paste0(getwd(), "/data/spamhamdocs.csv"))
  
}

#Used this to create the dataset.  It has been uploaded to github
#spamdir <- paste0(getwd(), "/data/spam")
#hamdir <- paste0(getwd(), "/data/easy_ham")
#compile_spamham(spamdir, hamdir)


#Downloads the pre-compiled dataset 
spamhamdocs <- read.csv("https://raw.githubusercontent.com/davidblumenstiel/Portfolio_2022/main/Spam%20Filter/data/spamhamdocs.csv")
spamhamdocs$X <- NULL

```

### Now that we've got our data loaded in, we need to separate out the words.  For this, we'll create a document term matrix.  This is essentially a count of what words occur and how many times they occur for each email.

```{r, message=FALSE, warning=FALSE}
#We need to get the data into a corpus object; this will first take a vector of the texts from each email (spamham[,1]), interperet it as a vector of documents (VectorSource()), then load it as a corpus object (Corpus()) 
spamhamcorp <- Corpus(VectorSource(spamhamdocs[,1]))


#Now, it needs to be cleaned.  Included in the corpus is a bunch of nonsense related to the email format, which tends to be fairly similar between spam and ham.  Below, it is purged.

spamhamcorp <- spamhamcorp %>%
  tm_map(content_transformer(tolower)) %>% #Changes all characters to lower case characters
  tm_map(removeWords, stopwords(kind = "en")) %>% #Removes stop-words (the english ones)
  tm_map(stemDocument) %>% #applies Porter's stemming algorithm; removes endings (e.g. created, creates -> create)
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) #This line got rid of alot of nonsense related to the email headers


#Creates the document term matrix
spamhamDTM <- DocumentTermMatrix(spamhamcorp) %>%
  removeSparseTerms(0.99) #We have alot of words (and other nonsense stuff) that just doesn't occur much.  This trims out anything that occurs infrequently.  Values approaching 1 allow sparcer terms to pass.  Even a value of 0.99 filters out about 150,000 terms (mostly nonsense)

#Sticks the result in a dataframe
spamham <- as.data.frame(as.matrix(spamhamDTM))

#Turns word frequencies into word occurance (e.g. 4->1, 0->0, 1->1)
spamham[spamham>1] <- 1

#Adds spam status to it.  
spamham <- cbind(spamhamdocs[,2], spamham)
colnames(spamham)[1] <- "is.spam"

#Makes factors for all variables
spamham <- as.data.frame(lapply(spamham, as.factor))

```

### Going forward, we'll need two different datasets: one for training our model, and one for testing it out afterwards.  Below, the data we have is divided between testing and training sets.


```{r}
#caTools includes an easy tool for this, which yields a logical vector with random trues and falses under the proportion specified.  One can use that to subset the main dataset

set.seed("1234567890")

set <- sample.split(spamham, SplitRatio = 0.80) 
train <- spamham[set == TRUE,]
test <- spamham[set == FALSE,]


#Seperates out spam and ham from train
spam <- subset(train, is.spam == TRUE)
ham <- subset(train, is.spam == FALSE)
```



### Now that we know which terms occur in each document; let's see what some common terms are.

```{r, fig.height=20,message=FALSE, warning=FALSE}

df <- as.data.frame(colSums(as.data.frame(lapply(spam, as.numeric))[-1]))
colnames(df) <- "value"
df$terms <- rownames(df)
df<- head(as.data.frame(df[order(-df$value),]),30)
spamplot <- ggplot(df, aes(x = reorder(df$terms, df$value), y = df$value)) + 
  geom_bar(stat = "identity") +
  xlab("Term") +
  ylab("Occurances") +
  labs(title = "Spam") +
  coord_flip()


df1 <- as.data.frame(colSums(as.data.frame(lapply(ham, as.numeric))[-1]))
colnames(df1) <- "value"
df1$terms <- rownames(df1)
df1<- head(as.data.frame(df1[order(-df$value),]),30)
hamplot <- ggplot(df, aes(x = reorder(df1$terms, df1$value), y = df1$value)) + 
  geom_bar(stat = "identity") +
  xlab("Term") +
  ylab("Occurances") +
  labs(title = "Ham") +
  coord_flip()

grid.arrange(spamplot, hamplot)

```

### It does seem like some of the most common terms are present in both types of email, although in different frequencies, e.g. date.  Also interestingly, all of the 30 most common ham terms start with letters A through E, which is not true for spam.  Let's see if the differences in term frequency are enough for the model to distinguish between the two.

### We're going to go with random forest to make the model.  In short, random forest utilizes multiple decision trees to build a model that is more accurate than any one tree could be.  One could also try otger types of models, but we'll stick with just one for now.

```{r}
set.seed("1234567890")

RFmodel <- randomForest(x = train[,-1], y = train[,1], ntree = 200)
RFmodel
```

### Easy enough to get the model using this method.  The function that creates the model predicts an error rate of 0.12%, but let's see what it actually is when we use that test data we set aside earlier.

```{r}
set.seed("1234567890")
prediction <- predict(RFmodel, test[,-1])

confusionMatrix(prediction, test$is.spam)
```

### It looks like the model still works pretty well with data it hasn't yet seen.  Using the testing dataset, the model was determined to be 99.67% accurate, only mislabling two pieces of ham as spam out of about 600 messages.  


### Some resources I used:

https://stackoverflow.com/questions/57173255/how-to-filter-one-list-by-whether-or-not-it-contains-a-string-from-another

https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf

https://www.r-bloggers.com/how-to-implement-random-forests-in-r/

https://www.guru99.com/r-random-forest-tutorial.html#1




