knitr::opts_chunk$set(echo = TRUE)
library("readtext")
library("tm")
library("dplyr")
library("ggplot2")
library("gridExtra")
library("caTools")
library("randomForest")
library("caret")
library("readr")
#Downloads the pre-compiled dataset
spamhamdocs <- read.csv("https://raw.githubusercontent.com/davidblumenstiel/Portfolio_2022/main/Spam%20Filter/data/spamhamdocs.csv")
knitr::opts_chunk$set(echo = TRUE)
library("readtext")
library("tm")
library("dplyr")
library("ggplot2")
library("gridExtra")
library("caTools")
library("randomForest")
library("caret")
library("readr")
#The function used to compile the data
#This won't actually run by default: the dataframe this creates is instead pre-loaded from github
compile_spamham <- function(spamdir, hamdir) {
#Initializes an empty dataframe
spamhamdocs <- data.frame(body = character(), is_spam = logical(), stringsAsFactors = FALSE)
#Returns a list of file names for all of the files in the folder where we keep the spam emails
filenames <- list.files(spamdir)
#Populates the dataframe with the spam messages, and labels them spam
i = 0
while (i < length(filenames)) {
i = i+1
spamhamdocs[nrow(spamhamdocs) + 1,1] <- suppressWarnings(readtext(paste0(spamdir,"\\",filenames[i])))[2]
spamhamdocs[nrow(spamhamdocs),2] <- TRUE
}
#Populates the dataframe with the ham messages, and labels them as not spam (same as above)
filenames <- list.files(hamdir)
i = 0
while (i < length(filenames)) {
i = i+1
spamhamdocs[nrow(spamhamdocs) + 1,1] <- suppressWarnings(readtext(paste0(hamdir,"\\",filenames[i])))[2]
spamhamdocs[nrow(spamhamdocs),2] <- FALSE
}
#writes the compiled spam/ham data to a csv
write.csv(spamhamdocs,
paste0(getwd(), "/data/spamhamdocs.csv"))
}
#Used this to create the dataset.  It has been uploaded to github
#spamdir <- paste0(getwd(), "/data/spam")
#hamdir <- paste0(getwd(), "/data/easy_ham")
#compile_spamham(spamdir, hamdir)
#Downloads the pre-compiled dataset
spamhamdocs <- read.csv("https://raw.githubusercontent.com/davidblumenstiel/Portfolio_2022/main/Spam%20Filter/data/spamhamdocs.csv")
spamhamdocs$X <- NULL
#We need to get the data into a corpus object; this will first take a vector of the texts from each email (spamham[,1]), interperet it as a vector of documents (VectorSource()), then load it as a corpus object (Corpus())
spamhamcorp <- Corpus(VectorSource(spamhamdocs[,1]))
#Now, it needs to be cleaned.  Included in the corpus is a bunch of nonsense related to the email format, which tends to be fairly similar between spam and ham.  Below, it is purged.
spamhamcorp <- spamhamcorp %>%
tm_map(content_transformer(tolower)) %>% #Changes all characters to lower case characters
tm_map(removeWords, stopwords(kind = "en")) %>% #Removes stop-words (the english ones)
tm_map(stemDocument) %>% #applies Porter's stemming algorithm; removes endings (e.g. created, creates -> create)
tm_map(removePunctuation) %>%
tm_map(removeNumbers) #This line got rid of alot of nonsense related to the email headers
library("SnowballC")
install.packages("SnowballC")
library("SnowballC")
#We need to get the data into a corpus object; this will first take a vector of the texts from each email (spamham[,1]), interperet it as a vector of documents (VectorSource()), then load it as a corpus object (Corpus())
spamhamcorp <- Corpus(VectorSource(spamhamdocs[,1]))
#Now, it needs to be cleaned.  Included in the corpus is a bunch of nonsense related to the email format, which tends to be fairly similar between spam and ham.  Below, it is purged.
spamhamcorp <- spamhamcorp %>%
tm_map(content_transformer(tolower)) %>% #Changes all characters to lower case characters
tm_map(removeWords, stopwords(kind = "en")) %>% #Removes stop-words (the english ones)
tm_map(stemDocument) %>% #applies Porter's stemming algorithm; removes endings (e.g. created, creates -> create)
tm_map(removePunctuation) %>%
tm_map(removeNumbers) #This line got rid of alot of nonsense related to the email headers
#Creates the document term matrix
spamhamDTM <- DocumentTermMatrix(spamhamcorp) %>%
removeSparseTerms(0.99) #We have alot of words (and other nonsense stuff) that just doesn't occur much.  This trims out anything that occurs infrequently.  Values approaching 1 allow sparcer terms to pass.  Even a value of 0.99 filters out about 150,000 terms (mostly nonsense)
#Sticks the result in a dataframe
spamham <- as.data.frame(as.matrix(spamhamDTM))
#Turns word frequencies into word occurance (e.g. 4->1, 0->0, 1->1)
spamham[spamham>1] <- 1
#Adds spam status to it.
spamham <- cbind(spamhamdocs[,2], spamham)
colnames(spamham)[1] <- "is.spam"
#Makes factors for all variables
spamham <- as.data.frame(lapply(spamham, as.factor))
knitr::opts_chunk$set(echo = TRUE)
library("readtext")
library("tm")
library("dplyr")
library("ggplot2")
library("gridExtra")
library("caTools")
library("randomForest")
library("caret")
library("readr")
library("SnowballC")
#The function used to compile the data
#This won't actually run by default: the dataframe this creates is instead pre-loaded from github
compile_spamham <- function(spamdir, hamdir) {
#Initializes an empty dataframe
spamhamdocs <- data.frame(body = character(), is_spam = logical(), stringsAsFactors = FALSE)
#Returns a list of file names for all of the files in the folder where we keep the spam emails
filenames <- list.files(spamdir)
#Populates the dataframe with the spam messages, and labels them spam
i = 0
while (i < length(filenames)) {
i = i+1
spamhamdocs[nrow(spamhamdocs) + 1,1] <- suppressWarnings(readtext(paste0(spamdir,"\\",filenames[i])))[2]
spamhamdocs[nrow(spamhamdocs),2] <- TRUE
}
#Populates the dataframe with the ham messages, and labels them as not spam (same as above)
filenames <- list.files(hamdir)
i = 0
while (i < length(filenames)) {
i = i+1
spamhamdocs[nrow(spamhamdocs) + 1,1] <- suppressWarnings(readtext(paste0(hamdir,"\\",filenames[i])))[2]
spamhamdocs[nrow(spamhamdocs),2] <- FALSE
}
#writes the compiled spam/ham data to a csv
write.csv(spamhamdocs,
paste0(getwd(), "/data/spamhamdocs.csv"))
}
#Used this to create the dataset.  It has been uploaded to github
#spamdir <- paste0(getwd(), "/data/spam")
#hamdir <- paste0(getwd(), "/data/easy_ham")
#compile_spamham(spamdir, hamdir)
#Downloads the pre-compiled dataset
spamhamdocs <- read.csv("https://raw.githubusercontent.com/davidblumenstiel/Portfolio_2022/main/Spam%20Filter/data/spamhamdocs.csv")
spamhamdocs$X <- NULL
#We need to get the data into a corpus object; this will first take a vector of the texts from each email (spamham[,1]), interperet it as a vector of documents (VectorSource()), then load it as a corpus object (Corpus())
spamhamcorp <- Corpus(VectorSource(spamhamdocs[,1]))
#Now, it needs to be cleaned.  Included in the corpus is a bunch of nonsense related to the email format, which tends to be fairly similar between spam and ham.  Below, it is purged.
spamhamcorp <- spamhamcorp %>%
tm_map(content_transformer(tolower)) %>% #Changes all characters to lower case characters
tm_map(removeWords, stopwords(kind = "en")) %>% #Removes stop-words (the english ones)
tm_map(stemDocument) %>% #applies Porter's stemming algorithm; removes endings (e.g. created, creates -> create)
tm_map(removePunctuation) %>%
tm_map(removeNumbers) #This line got rid of alot of nonsense related to the email headers
#Creates the document term matrix
spamhamDTM <- DocumentTermMatrix(spamhamcorp) %>%
removeSparseTerms(0.99) #We have alot of words (and other nonsense stuff) that just doesn't occur much.  This trims out anything that occurs infrequently.  Values approaching 1 allow sparcer terms to pass.  Even a value of 0.99 filters out about 150,000 terms (mostly nonsense)
#Sticks the result in a dataframe
spamham <- as.data.frame(as.matrix(spamhamDTM))
#Turns word frequencies into word occurance (e.g. 4->1, 0->0, 1->1)
spamham[spamham>1] <- 1
#Adds spam status to it.
spamham <- cbind(spamhamdocs[,2], spamham)
colnames(spamham)[1] <- "is.spam"
#Makes factors for all variables
spamham <- as.data.frame(lapply(spamham, as.factor))
#caTools includes an easy tool for this, which yields a logical vector with random trues and falses under the proportion specified.  One can use that to subset the main dataset
set.seed("1234567890")
set <- sample.split(spamham, SplitRatio = 0.80)
train <- spamham[set == TRUE,]
test <- spamham[set == FALSE,]
#Seperates out spam and ham from train
spam <- subset(train, is.spam == TRUE)
ham <- subset(train, is.spam == FALSE)
df <- as.data.frame(colSums(as.data.frame(lapply(spam, as.numeric))[-1]))
colnames(df) <- "value"
df$terms <- rownames(df)
df<- head(as.data.frame(df[order(-df$value),]),30)
spamplot <- ggplot(df, aes(x = reorder(df$terms, df$value), y = df$value)) +
geom_bar(stat = "identity") +
xlab("Term") +
ylab("Occurances") +
labs(title = "Spam") +
coord_flip()
df1 <- as.data.frame(colSums(as.data.frame(lapply(ham, as.numeric))[-1]))
colnames(df1) <- "value"
df1$terms <- rownames(df1)
df1<- head(as.data.frame(df1[order(-df$value),]),30)
hamplot <- ggplot(df, aes(x = reorder(df1$terms, df1$value), y = df1$value)) +
geom_bar(stat = "identity") +
xlab("Term") +
ylab("Occurances") +
labs(title = "Ham") +
coord_flip()
grid.arrange(spamplot, hamplot)
set.seed("1234567890")
RFmodel <- randomForest(x = train[,-1], y = train[,1], ntree = 200)
RFmodel
set.seed("1234567890")
prediction <- predict(RFmodel, test[,-1])
confusionMatrix(prediction, test$is.spam)
